{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import json as json\n",
    "import seaborn as sbn\n",
    "# import pyspark as py\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "# import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: 465476, 2: 64718, 4: 149331, 3: 97031, 1: 107080}\n",
      "883636\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "d=dict()\n",
    "pd_read=pd.read_json('/ichec/work/mucom001c/Amazon/review/AMAZON_FASHION.json',lines=True)\n",
    "pd1=pd_read.values.tolist()\n",
    "for i in pd1:\n",
    "    t=i[0]\n",
    "    if t not in d:d[t]=1\n",
    "    else:\n",
    "        d[t]+=1\n",
    "    count+=1\n",
    "print(d)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall             int64\n",
       "verified             bool\n",
       "reviewTime         object\n",
       "reviewerID         object\n",
       "asin               object\n",
       "reviewerName       object\n",
       "reviewText         object\n",
       "summary            object\n",
       "unixReviewTime      int64\n",
       "vote              float64\n",
       "style              object\n",
       "image              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_read.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>style</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>10 20, 2014</td>\n",
       "      <td>A1D4G1SNUZWQOT</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Tracy</td>\n",
       "      <td>Exactly what I needed.</td>\n",
       "      <td>perfect replacements!!</td>\n",
       "      <td>1413763200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>09 28, 2014</td>\n",
       "      <td>A3DDWDH9PX2YX2</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Sonja Lau</td>\n",
       "      <td>I agree with the other review, the opening is ...</td>\n",
       "      <td>I agree with the other review, the opening is ...</td>\n",
       "      <td>1411862400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>08 25, 2014</td>\n",
       "      <td>A2MWC41EW7XL15</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Kathleen</td>\n",
       "      <td>Love these... I am going to order another pack...</td>\n",
       "      <td>My New 'Friends' !!</td>\n",
       "      <td>1408924800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>08 24, 2014</td>\n",
       "      <td>A2UH2QQ275NV45</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Jodi Stoner</td>\n",
       "      <td>too tiny an opening</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>1408838400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>07 27, 2014</td>\n",
       "      <td>A89F3LQADZBS5</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Alexander D.</td>\n",
       "      <td>Okay</td>\n",
       "      <td>Three Stars</td>\n",
       "      <td>1406419200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime      reviewerID        asin  reviewerName  \\\n",
       "0        5      True  10 20, 2014  A1D4G1SNUZWQOT  7106116521         Tracy   \n",
       "1        2      True  09 28, 2014  A3DDWDH9PX2YX2  7106116521     Sonja Lau   \n",
       "2        4     False  08 25, 2014  A2MWC41EW7XL15  7106116521      Kathleen   \n",
       "3        2      True  08 24, 2014  A2UH2QQ275NV45  7106116521   Jodi Stoner   \n",
       "4        3     False  07 27, 2014   A89F3LQADZBS5  7106116521  Alexander D.   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0                             Exactly what I needed.   \n",
       "1  I agree with the other review, the opening is ...   \n",
       "2  Love these... I am going to order another pack...   \n",
       "3                                too tiny an opening   \n",
       "4                                               Okay   \n",
       "\n",
       "                                             summary  unixReviewTime  vote  \\\n",
       "0                             perfect replacements!!      1413763200   NaN   \n",
       "1  I agree with the other review, the opening is ...      1411862400   3.0   \n",
       "2                                My New 'Friends' !!      1408924800   NaN   \n",
       "3                                          Two Stars      1408838400   NaN   \n",
       "4                                        Three Stars      1406419200   NaN   \n",
       "\n",
       "  style image  \n",
       "0   NaN   NaN  \n",
       "1   NaN   NaN  \n",
       "2   NaN   NaN  \n",
       "3   NaN   NaN  \n",
       "4   NaN   NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_read.shape\n",
    "\n",
    "import array as arr\n",
    "\n",
    "pd_read=pd.read_json('/ichec/work/mucom001c/Amazon/review/AMAZON_FASHION.json',lines=True)\n",
    "pd_read.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Exactly what I needed.</td>\n",
       "      <td>perfect replacements!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I agree with the other review, the opening is ...</td>\n",
       "      <td>I agree with the other review, the opening is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Love these... I am going to order another pack...</td>\n",
       "      <td>My New 'Friends' !!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>too tiny an opening</td>\n",
       "      <td>Two Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Okay</td>\n",
       "      <td>Three Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883631</th>\n",
       "      <td>5</td>\n",
       "      <td>I absolutely love this dress!!  It's sexy and ...</td>\n",
       "      <td>I absolutely love this dress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883632</th>\n",
       "      <td>5</td>\n",
       "      <td>I'm 5'6 175lbs. I'm on the tall side. I wear a...</td>\n",
       "      <td>I wear a large and ordered a large and it stil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883633</th>\n",
       "      <td>3</td>\n",
       "      <td>Too big in the chest area!</td>\n",
       "      <td>Three Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883634</th>\n",
       "      <td>3</td>\n",
       "      <td>Too clear in the back, needs lining</td>\n",
       "      <td>Three Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883635</th>\n",
       "      <td>5</td>\n",
       "      <td>Ordered and was slightly small. Worked with th...</td>\n",
       "      <td>The quality is excellent and it is so cute</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>883636 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall                                         reviewText  \\\n",
       "0             5                             Exactly what I needed.   \n",
       "1             2  I agree with the other review, the opening is ...   \n",
       "2             4  Love these... I am going to order another pack...   \n",
       "3             2                                too tiny an opening   \n",
       "4             3                                               Okay   \n",
       "...         ...                                                ...   \n",
       "883631        5  I absolutely love this dress!!  It's sexy and ...   \n",
       "883632        5  I'm 5'6 175lbs. I'm on the tall side. I wear a...   \n",
       "883633        3                         Too big in the chest area!   \n",
       "883634        3                Too clear in the back, needs lining   \n",
       "883635        5  Ordered and was slightly small. Worked with th...   \n",
       "\n",
       "                                                  summary  \n",
       "0                                  perfect replacements!!  \n",
       "1       I agree with the other review, the opening is ...  \n",
       "2                                     My New 'Friends' !!  \n",
       "3                                               Two Stars  \n",
       "4                                             Three Stars  \n",
       "...                                                   ...  \n",
       "883631                       I absolutely love this dress  \n",
       "883632  I wear a large and ordered a large and it stil...  \n",
       "883633                                        Three Stars  \n",
       "883634                                        Three Stars  \n",
       "883635         The quality is excellent and it is so cute  \n",
       "\n",
       "[883636 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data=pd_read.drop(['verified','reviewerID','asin','reviewTime','reviewerName','unixReviewTime','vote','style','image'],axis=1)\n",
    "review_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa22b4b5700>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEGCAYAAACpXNjrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAASW0lEQVR4nO3df7CmdV3/8efLXQjTCJQT4S61TO1Um5XmfpGibzU4wWLmUoMMTspmFM0EDY79wm9NlsV8bcrMzJgYIdhqAkrNzShiEDUdEc4CokBMJ0LZHWw3QBAbdaB3f9yfZe8O5xwO+rnv65yzz8fMPXtd7+vH533u0Xlx/bivK1WFJEk9PWvoBiRJa4/hIknqznCRJHVnuEiSujNcJEndrR+6gZXimGOOqU2bNg3dhiStKrt37/7PqpqZXzdcmk2bNjE7Ozt0G5K0qiT59EJ1T4tJkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrrzF/qS1Mkf/+LfDd3CRFzw1h97xtt45CJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6m7i4ZJkXZLbkry/zZ+Q5ONJ5pJcneTwVv+aNj/Xlm8a28cbW/2eJKeN1be12lySi8bqC44hSZqOaRy5XAjcPTb/u8DbqupbgYeBc1v9XODhVn9bW48kW4Czge8EtgF/0gJrHfBO4HRgC/Dqtu5SY0iSpmCi4ZJkI/CjwLvafIBTgL9pq1wJnNGmt7d52vKXtfW3A1dV1Zeq6t+BOeDE9pmrqnur6svAVcD2pxlDkjQFkz5y+UPgV4D/bvPPBz5XVY+3+T3Ahja9AbgfoC1/pK3/ZH3eNovVlxrjf0lyXpLZJLP79+//Sv9GSdI8EwuXJK8A9lXV7kmN8dWqqkuramtVbZ2ZmRm6HUlaM9ZPcN8nA69M8nLgCOBI4O3AUUnWtyOLjcDetv5e4HhgT5L1wNcDD47VDxjfZqH6g0uMIUmagokduVTVG6tqY1VtYnRB/gNV9ZPAjcCZbbUdwPva9K42T1v+gaqqVj+73U12ArAZuBm4Bdjc7gw7vI2xq22z2BiSpCkY4ncuvwq8Ickco+sjl7X6ZcDzW/0NwEUAVXUncA1wF/CPwPlV9UQ7KrkAuI7R3WjXtHWXGkOSNAWTPC32pKr6IPDBNn0vozu95q/zReBVi2x/MXDxAvVrgWsXqC84hiRpOvyFviSpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6m5i4ZLkiCQ3J/lEkjuT/Farn5Dk40nmklyd5PBW/5o2P9eWbxrb1xtb/Z4kp43Vt7XaXJKLxuoLjiFJmo5JHrl8CTilqr4HeBGwLclJwO8Cb6uqbwUeBs5t658LPNzqb2vrkWQLcDbwncA24E+SrEuyDngncDqwBXh1W5clxpAkTcHEwqVGHmuzh7VPAacAf9PqVwJntOntbZ62/GVJ0upXVdWXqurfgTngxPaZq6p7q+rLwFXA9rbNYmNIkqZgotdc2hHG7cA+4Hrg34DPVdXjbZU9wIY2vQG4H6AtfwR4/nh93jaL1Z+/xBjz+zsvyWyS2f379381f6okacxEw6WqnqiqFwEbGR1pfPskx3umqurSqtpaVVtnZmaGbkeS1oyp3C1WVZ8DbgS+Dzgqyfq2aCOwt03vBY4HaMu/HnhwvD5vm8XqDy4xhiRpCiZ5t9hMkqPa9LOBHwHuZhQyZ7bVdgDva9O72jxt+Qeqqlr97HY32QnAZuBm4BZgc7sz7HBGF/13tW0WG0OSNAXrn36Vr9hxwJXtrq5nAddU1fuT3AVcleR3gNuAy9r6lwF/nmQOeIhRWFBVdya5BrgLeBw4v6qeAEhyAXAdsA64vKrubPv61UXGkCRNwcTCparuAF68QP1eRtdf5te/CLxqkX1dDFy8QP1a4NrljiFJmg5/oS9J6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktTdssIlyQ3LqUmSBE/zC/0kRwBfCxyT5GggbdGRLPIYe0mSnu7xLz8HvB54AbCbg+HyKPDHE+xLkrSKLRkuVfV24O1JfqGq3jGlniRJq9yyHlxZVe9I8v3ApvFtqmrnhPqSJK1iywqXJH8OfAtwO/BEKxdguEiSnmK5j9zfCmxpL+KSJGlJy/2dy6eAb5xkI5KktWO5Ry7HAHcluRn40oFiVb1yIl1Jkla15YbLb06yCUnS2rLcu8U+NOlGJElrx3LvFvs8o7vDAA4HDgO+UFVHTqoxSdLqtdwjl687MJ0kwHbgpEk1JUla3Z7xU5Fr5G+B0ybQjyRpDVjuabGfGJt9FqPfvXxxIh1Jkla95d4t9mNj048D9zE6NSZJ0lMs95rL6ybdiCRp7Vjuy8I2Jnlvkn3t8+4kGyfdnCRpdVruBf0/A3Yxeq/LC4C/azVJkp5iueEyU1V/VlWPt88VwMwE+5IkrWLLDZcHk7wmybr2eQ3w4CQbkyStXssNl58GzgI+CzwAnAn81IR6kiStcsu9FfnNwI6qehggyfOA32cUOpIk/S/LPXL57gPBAlBVDwEvnkxLkqTVbrnh8qwkRx+YaUcuyz3qkSQdYpYbEG8FPpbkr9v8q4CLJ9OSJGm1W+4v9HcmmQVOaaWfqKq7JteWJGk1W/aprRYmBook6Wk940fuL1eS45PcmOSuJHcmubDVn5fk+iT/2v49utWT5I+SzCW5I8n3ju1rR1v/X5PsGKu/JMkn2zZ/1N41s+gYkqTpmFi4MHp68i9W1RZGLxY7P8kW4CLghqraDNzQ5gFOBza3z3nAJfDkzQNvAl4KnAi8aSwsLgF+dmy7ba2+2BiSpCmYWLhU1QNVdWub/jxwN7CB0aP6r2yrXQmc0aa3Azvby8huAo5Kchyjl5JdX1UPtduhrwe2tWVHVtVNVVXAznn7WmgMSdIUTPLI5UlJNjH6XczHgWOr6oG26LPAsW16A3D/2GZ7Wm2p+p4F6iwxxvy+zksym2R2//79z/wPkyQtaOLhkuS5wLuB11fVo+PL2hFHTXL8pcaoqkuramtVbZ2Z8TmcktTLRMMlyWGMguUvq+o9rfwf7ZQW7d99rb4XOH5s842ttlR94wL1pcaQJE3BJO8WC3AZcHdV/cHYol3AgTu+dgDvG6uf0+4aOwl4pJ3aug44NcnR7UL+qcB1bdmjSU5qY50zb18LjSFJmoJJPsLlZOC1wCeT3N5q/w94C3BNknOBTzN62jLAtcDLgTngv4DXweg5Zkl+G7ilrffm9mwzgJ8HrgCeDfxD+7DEGJKkKZhYuFTVR4AssvhlC6xfwPmL7Oty4PIF6rPACxeoP7jQGJKk6ZjK3WKSpEOL4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpu0m+iVLSIeBDP/hDQ7cwET/04Q8N3cKq5pGLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1N3EXnOc5HLgFcC+qnphqz0PuBrYBNwHnFVVDycJ8Hbg5cB/AT9VVbe2bXYAv952+ztVdWWrvwS4Ang2cC1wYVXVYmN8NX/LS35551ez+Yq0+/fOGboFSWvYJI9crgC2zatdBNxQVZuBG9o8wOnA5vY5D7gEngyjNwEvBU4E3pTk6LbNJcDPjm237WnGkCRNycTCpao+DDw0r7wduLJNXwmcMVbfWSM3AUclOQ44Dbi+qh5qRx/XA9vasiOr6qaqKmDnvH0tNIYkaUqmfc3l2Kp6oE1/Fji2TW8A7h9bb0+rLVXfs0B9qTGeIsl5SWaTzO7fv/8r+HMkSQsZ7IJ+O+KoIceoqkuramtVbZ2ZmZlkK5J0SJnYBf1F/EeS46rqgXZqa1+r7wWOH1tvY6vtBX54Xv2Drb5xgfWXGkPq5uR3nDx0CxPx0V/46NAtaI2Y9pHLLmBHm94BvG+sfk5GTgIeaae2rgNOTXJ0u5B/KnBdW/ZokpPanWbnzNvXQmNIkqZkkrci/xWjo45jkuxhdNfXW4BrkpwLfBo4q61+LaPbkOcY3Yr8OoCqeijJbwO3tPXeXFUHbhL4eQ7eivwP7cMSY0iSpmRi4VJVr15k0csWWLeA8xfZz+XA5QvUZ4EXLlB/cKExJEnT4y/0JUndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO6m/VRkrWKfefN3Dd3CRHzTb3xy6BakNccjF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd2s2XJJsS3JPkrkkFw3djyQdStZkuCRZB7wTOB3YArw6yZZhu5KkQ8eaDBfgRGCuqu6tqi8DVwHbB+5Jkg4Zqaqhe+guyZnAtqr6mTb/WuClVXXBvPXOA85rs98G3DPVRp/qGOA/B+5hpfC7OMjv4iC/i4NWynfxzVU1M7+4fohOVoqquhS4dOg+DkgyW1Vbh+5jJfC7OMjv4iC/i4NW+nexVk+L7QWOH5vf2GqSpClYq+FyC7A5yQlJDgfOBnYN3JMkHTLW5Gmxqno8yQXAdcA64PKqunPgtpZjxZyiWwH8Lg7yuzjI7+KgFf1drMkL+pKkYa3V02KSpAEZLpKk7gyXFSDJ5Un2JfnU0L0MLcnxSW5McleSO5NcOHRPQ0lyRJKbk3yifRe/NXRPQ0qyLsltSd4/dC9DS3Jfkk8muT3J7ND9LMRrLitAkh8EHgN2VtULh+5nSEmOA46rqluTfB2wGzijqu4auLWpSxLgOVX1WJLDgI8AF1bVTQO3NogkbwC2AkdW1SuG7mdISe4DtlbVSvgR5YI8clkBqurDwEND97ESVNUDVXVrm/48cDewYdiuhlEjj7XZw9rnkPyvwSQbgR8F3jV0L1oew0UrVpJNwIuBjw/byXDaqaDbgX3A9VV1qH4Xfwj8CvDfQzeyQhTwT0l2t8dYrTiGi1akJM8F3g28vqoeHbqfoVTVE1X1IkZPmTgxySF32jTJK4B9VbV76F5WkB+oqu9l9OT389up9RXFcNGK064vvBv4y6p6z9D9rARV9TngRmDb0L0M4GTgle06w1XAKUn+YtiWhlVVe9u/+4D3MnoS/IpiuGhFaRexLwPurqo/GLqfISWZSXJUm3428CPAvwzb1fRV1RuramNVbWL0KKcPVNVrBm5rMEme0252IclzgFOBFXenqeGyAiT5K+BjwLcl2ZPk3KF7GtDJwGsZ/dfp7e3z8qGbGshxwI1J7mD0vLzrq+qQvw1XHAt8JMkngJuBv6+qfxy4p6fwVmRJUnceuUiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0VaY5J8MMnWNn1fkmOG7kmHHsNFWmUy4v93taL5P1BpCpK8Icmn2uf1Sd6S5Pyx5b+Z5Jfa9C8nuSXJHQfe4ZJkU5J7kuxk9Gvs45NckmTWd71oJVo/dAPSWpfkJcDrgJcCYfSU59cwetLvO9tqZwGnJTkV2MzoWVEBdrWHEn6m1XcceJ9Lkl+rqoeSrANuSPLdVXXHFP80aVGGizR5PwC8t6q+AJDkPcD/Bb4hyQuAGeDhqrq/vXnzVOC2tu1zGYXKZ4BPz3tR2FntcevrGT0qZgtguGhFMFyk4fw1cCbwjcDVrRbg/1fVn46v2N5t84Wx+ROAXwL+T1U9nOQK4IjJtywtj9dcpMn7Z+CMJF/bnmL74612NaOn/J7JKGgArgN+ur3PhiQbknzDAvs8klHYPJLkWEbv9ZBWDI9cpAmrqlvbkcXNrfSuqroNoD06fW9VPdDW/ack3wF8bPT2AR5jdH3miXn7/ESS2xg9gv9+4KPT+Fuk5fKpyJKk7jwtJknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKm7/wFnnE7qnO5NHAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sbn.countplot(review_data['overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall          0\n",
      "reviewText    1233\n",
      "summary        533\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(review_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_clean( rev, remove_stopwords=True): \n",
    "#     new_text = re.sub(\"[^a-zA-Z]\",\" \", rev)\n",
    "#     words = new_text.lower().split()\n",
    "#     if remove_stopwords:\n",
    "#         sts = set(stopwords.words(\"english\"))\n",
    "#         words = [w for w in words if not w in sts]\n",
    "#     ary=[]\n",
    "#     eng_stemmer = english_stemmer \n",
    "#     for word in words:\n",
    "#         ary.append(eng_stemmer.stem(word))\n",
    "#     return(ary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (883636, 3)\n",
      "Drop Nulls: (881900, 3)\n",
      "overall       0\n",
      "reviewText    0\n",
      "summary       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print (\"Original:\", review_data.shape)\n",
    "# review_data_dd = review_data.drop_duplicates()\n",
    "# dd = review_data_dd.reset_index(drop=True)\n",
    "# print (\"Drop Dupicates:\", dd.shape)\n",
    "review_data = review_data.dropna()\n",
    "review_data = review_data.reset_index(drop=True)\n",
    "print (\"Drop Nulls:\", review_data.shape)\n",
    "print(review_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review_data[\"reviewcomment\"] = review_data[\"reviewText\"] + review_data[\"summary\"]\n",
    "review_data['reviewcomment'] = review_data[['reviewText', 'summary']].astype(str).apply(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewcomment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Exactly what I needed. perfect replacements!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I agree with the other review, the opening is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Love these... I am going to order another pack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>too tiny an opening Two Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Okay Three Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881895</th>\n",
       "      <td>5</td>\n",
       "      <td>I absolutely love this dress!!  It's sexy and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881896</th>\n",
       "      <td>5</td>\n",
       "      <td>I'm 5'6 175lbs. I'm on the tall side. I wear a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881897</th>\n",
       "      <td>3</td>\n",
       "      <td>Too big in the chest area! Three Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881898</th>\n",
       "      <td>3</td>\n",
       "      <td>Too clear in the back, needs lining Three Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881899</th>\n",
       "      <td>5</td>\n",
       "      <td>Ordered and was slightly small. Worked with th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>881900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall                                      reviewcomment\n",
       "0             5      Exactly what I needed. perfect replacements!!\n",
       "1             2  I agree with the other review, the opening is ...\n",
       "2             4  Love these... I am going to order another pack...\n",
       "3             2                      too tiny an opening Two Stars\n",
       "4             3                                   Okay Three Stars\n",
       "...         ...                                                ...\n",
       "881895        5  I absolutely love this dress!!  It's sexy and ...\n",
       "881896        5  I'm 5'6 175lbs. I'm on the tall side. I wear a...\n",
       "881897        3             Too big in the chest area! Three Stars\n",
       "881898        3    Too clear in the back, needs lining Three Stars\n",
       "881899        5  Ordered and was slightly small. Worked with th...\n",
       "\n",
       "[881900 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_list=review_data.drop(['reviewText','summary'],axis=1)\n",
    "review_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data_list = review_data_list['reviewcomment'].values.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I agree with the other review, the opening is too small.  I almost bent the hook on some very expensive earrings trying to get these up higher than just the end so they're not seen.  Would not buy again but for the price, not sending back. I agree with the other review, the opening is ...\n"
     ]
    }
   ],
   "source": [
    "print((review_data_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "\n",
    "token_list = list()\n",
    "\n",
    "for i in (review_data_list):\n",
    "    cleaned = re.sub('\\W+', ' ', i).lower()\n",
    "        # remove all single characters\n",
    "    cleaned = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', cleaned)\n",
    " \n",
    "    # Remove single characters from the start\n",
    "    cleaned = re.sub(r'\\^[a-zA-Z]\\s+', ' ', cleaned) \n",
    " \n",
    "    # Substituting multiple spaces with single space\n",
    "    cleaned= re.sub(r'\\s+', ' ', cleaned, flags=re.I)\n",
    " \n",
    "    # Removing prefixed 'b'\n",
    "    cleaned = re.sub(r'^b\\s+', '', cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    token_list.append(word_tokenize(str(cleaned)))\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['exactly', 'what', 'needed', 'perfect', 'replacements'], ['i', 'agree', 'with', 'the', 'other', 'review', 'the', 'opening', 'is', 'too', 'small', 'almost', 'bent', 'the', 'hook', 'on', 'some', 'very', 'expensive', 'earrings', 'trying', 'to', 'get', 'these', 'up', 'higher', 'than', 'just', 'the', 'end', 'so', 'they', 're', 'not', 'seen', 'would', 'not', 'buy', 'again', 'but', 'for', 'the', 'price', 'not', 'sending', 'back', 'agree', 'with', 'the', 'other', 'review', 'the', 'opening', 'is'], ['love', 'these', 'am', 'going', 'to', 'order', 'another', 'pack', 'to', 'keep', 'in', 'work', 'someone', 'including', 'myself', 'is', 'always', 'losing', 'the', 'back', 'to', 'an', 'earring', 'don', 'understand', 'why', 'all', 'fish', 'hook', 'earrings', 'don', 'have', 'them', 'just', 'wish', 'that', 'they', 'were', 'tiny', 'bit', 'longer', 'my', 'new', 'friends'], ['too', 'tiny', 'an', 'opening', 'two', 'stars'], ['okay', 'three', 'stars'], ['exactly', 'what', 'wanted', 'five', 'stars'], ['these', 'little', 'plastic', 'backs', 'work', 'great', 'no', 'more', 'loosing', 'hook', 'earrings', 'wish', 'had', 'ordered', 'these', 'sooner', 'before', 'had', 'lost', 'some', 'of', 'my', 'favorite', 'earrings', 'works', 'great'], ['mother', 'in', 'law', 'wanted', 'it', 'as', 'present', 'for', 'her', 'sister', 'she', 'liked', 'it', 'and', 'said', 'it', 'would', 'work', 'bought', 'as', 'present'], ['item', 'is', 'of', 'good', 'quality', 'looks', 'great', 'too', 'but', 'it', 'does', 'not', 'fit', '100', 'but', 'it', 'can', 'be', 'stretched', 'to', 'fit', 'them', 'if', 'you', 'carefully', 'push', 'bottom', 'of', 'case', 'with', 'your', 'fingers', 'then', 'shove', 'in', 'pack', 'of', '100', 'max', 'and', 'level', 'it', 'out', 'as', 'you', 'close', 'case', 'stretching', 'case', 'closed', 'leave', 'pk', 'in', 'it', 'for', 'month', 'or', 'so', 'was', 'la', 'buxton', 'is', 'usually', 'good', 'quality', 'product', 'buxton', 'heiress', 'collection'], ['i', 'had', 'used', 'my', 'last', 'el', 'cheapo', 'fake', 'leather', 'cigarette', 'case', 'for', 'seven', 'years', 'it', 'still', 'closed', 'completely', 'but', 'the', 'plastic', 'made', 'to', 'look', 'like', 'leather', 'was', 'literally', 'falling', 'off', 'so', 'it', 'was', 'time', 'for', 'new', 'one', 'cigarette', 'cases', 'for', 'kings', 'size', 'cigs', 'are', 'not', 'easy', 'to', 'come', 'by', 'these', 'days', 'discovered', 'but', 'was', 'thrilled', 'to', 'find', 'this', 'one', 'on', 'amazon', 'it', 'was', 'great', 'price', 'real', 'leather', 'and', 'even', 'had', 'the', 'cool', 'zipper', 'pouch', 'on', 'the', 'back', 'was', 'so', 'excited', 'to', 'get', 'my', 'case', 'and', 'toss', 'that', 'other', 'one', 'well', 'within', 'three', 'days', 'one', 'of', 'the', 'gold', 'clasps', 'literally', 'broke', 'off', 'couldn', 'believe', 'it', 'tried', 'to', 'super', 'glue', 'it', 'back', 'on', 'and', 'was', 'not', 'successful', 'so', 'still', 'use', 'the', 'case', 'but', 'it', 'doesn', 'close', 'securely', 'was', 'very', 'disappointed', 'that', 'my', '3', '00', 'plastic', 'one', 'lasted', '7', 'years', 'and', 'this', 'real', 'nice', 'leather', 'one', 'lasted', '3', 'days', 'but', 'still', 'love', 'the', 'zipper', 'pouch', 'on', 'the', 'back', 'it', 'great', 'for', 'the', 'spare', 'key', 'to', 'my', 'car', 'because', 'will', 'not', 'go', 'anywhere', 'without', 'my', 'cigarettes', 'top', 'clasp', 'broke', 'within', '3', 'days']]\n"
     ]
    }
   ],
   "source": [
    "print(token_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list_copy = token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "881900"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_list_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "from nltk import PorterStemmer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Stop_Words(Sent):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_words = []\n",
    "    for w in Sent:\n",
    "        if w not in stop_words:\n",
    "            filtered_words.append(w)\n",
    "    return stop_words,filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=list()\n",
    "# count=0\n",
    "for word in token_list_copy:\n",
    "#     if count==1000:\n",
    "#         print(x1)\n",
    "#         break\n",
    "#     else:\n",
    "    x,y=Remove_Stop_Words(word)\n",
    "    x1.append(y)\n",
    "#         count+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words,filtered_words = Remove_Stop_Words(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilaise stemmer\n",
    "\n",
    "porter_stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem raw words with noise\n",
    "raw_words = (x1)\n",
    "print(raw_words[0:10])\n",
    "stemmed_words = [porter_stemmer.stem(word =' '.join(word)) for word in raw_words]\n",
    "stemdf = pd.DataFrame({'raw_word': raw_words,'stemmed_word': stemmed_words})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            raw_word  \\\n",
      "0           [exactly, needed, perfect, replacements]   \n",
      "1  [agree, review, opening, small, almost, bent, ...   \n",
      "2  [love, going, order, another, pack, keep, work...   \n",
      "3                        [tiny, opening, two, stars]   \n",
      "4                               [okay, three, stars]   \n",
      "\n",
      "                                        stemmed_word  \n",
      "0                      exactly needed perfect replac  \n",
      "1  agree review opening small almost bent hook ex...  \n",
      "2  love going order another pack keep work someon...  \n",
      "3                              tiny opening two star  \n",
      "4                                    okay three star  \n"
     ]
    }
   ],
   "source": [
    "print(stemdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             exactly needed perfect replac\n",
       "1         agree review opening small almost bent hook ex...\n",
       "2         love going order another pack keep work someon...\n",
       "3                                     tiny opening two star\n",
       "4                                           okay three star\n",
       "                                ...                        \n",
       "881895    absolutely love dress sexy comfortable split b...\n",
       "881896    5 6 175lbs tall side wear large ordered large ...\n",
       "881897                            big chest area three star\n",
       "881898                   clear back needs lining three star\n",
       "881899    ordered slightly small worked company gracious...\n",
       "Name: stemmed_word, Length: 881900, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemdf['stemmed_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  \n",
    "  pos_counts = Counter()\n",
    "\n",
    "\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "# ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lemmatizing_Words(Words):\n",
    "    Lm = WordNetLemmatizer()\n",
    "    Lemmatized_Words = []\n",
    "    for word in Words:\n",
    "        Lemmatized_Words.append(Lm.lemmatize(word,get_part_of_speech(word)))\n",
    "    return Lemmatized_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=list()\n",
    "for word in x1:\n",
    "#     print(word)\n",
    "    x_temp = Lemmatizing_Words(word)\n",
    "    x2.append(x_temp)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['exactly', 'needed', 'perfect', 'replacements'], ['agree', 'review', 'opening', 'small', 'almost', 'bent', 'hook', 'expensive', 'earrings', 'trying', 'get', 'higher', 'end', 'seen', 'would', 'buy', 'price', 'sending', 'back', 'agree', 'review', 'opening'], ['love', 'going', 'order', 'another', 'pack', 'keep', 'work', 'someone', 'including', 'always', 'losing', 'back', 'earring', 'understand', 'fish', 'hook', 'earrings', 'wish', 'tiny', 'bit', 'longer', 'new', 'friends'], ['tiny', 'opening', 'two', 'stars'], ['okay', 'three', 'stars'], ['exactly', 'wanted', 'five', 'stars'], ['little', 'plastic', 'backs', 'work', 'great', 'loosing', 'hook', 'earrings', 'wish', 'ordered', 'sooner', 'lost', 'favorite', 'earrings', 'works', 'great'], ['mother', 'law', 'wanted', 'present', 'sister', 'liked', 'said', 'would', 'work', 'bought', 'present'], ['item', 'good', 'quality', 'looks', 'great', 'fit', '100', 'stretched', 'fit', 'carefully', 'push', 'bottom', 'case', 'fingers', 'shove', 'pack', '100', 'max', 'level', 'close', 'case', 'stretching', 'case', 'closed', 'leave', 'pk', 'month', 'la', 'buxton', 'usually', 'good', 'quality', 'product', 'buxton', 'heiress', 'collection'], ['used', 'last', 'el', 'cheapo', 'fake', 'leather', 'cigarette', 'case', 'seven', 'years', 'still', 'closed', 'completely', 'plastic', 'made', 'look', 'like', 'leather', 'literally', 'falling', 'time', 'new', 'one', 'cigarette', 'cases', 'kings', 'size', 'cigs', 'easy', 'come', 'days', 'discovered', 'thrilled', 'find', 'one', 'amazon', 'great', 'price', 'real', 'leather', 'even', 'cool', 'zipper', 'pouch', 'back', 'excited', 'get', 'case', 'toss', 'one', 'well', 'within', 'three', 'days', 'one', 'gold', 'clasps', 'literally', 'broke', 'believe', 'tried', 'super', 'glue', 'back', 'successful', 'still', 'use', 'case', 'close', 'securely', 'disappointed', '3', '00', 'plastic', 'one', 'lasted', '7', 'years', 'real', 'nice', 'leather', 'one', 'lasted', '3', 'days', 'still', 'love', 'zipper', 'pouch', 'back', 'great', 'spare', 'key', 'car', 'go', 'anywhere', 'without', 'cigarettes', 'top', 'clasp', 'broke', 'within', '3', 'days']]\n"
     ]
    }
   ],
   "source": [
    "print(x1[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['exactly', 'need', 'perfect', 'replacement'], ['agree', 'review', 'opening', 'small', 'almost', 'bend', 'hook', 'expensive', 'earring', 'try', 'get', 'high', 'end', 'see', 'would', 'buy', 'price', 'send', 'back', 'agree', 'review', 'opening'], ['love', 'go', 'order', 'another', 'pack', 'keep', 'work', 'someone', 'include', 'always', 'lose', 'back', 'earring', 'understand', 'fish', 'hook', 'earring', 'wish', 'tiny', 'bit', 'long', 'new', 'friend'], ['tiny', 'opening', 'two', 'star']]\n"
     ]
    }
   ],
   "source": [
    "print(x2[0:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "881900\n"
     ]
    }
   ],
   "source": [
    "print (len(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "881900"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2_str = [' '.join(i) for i in x2] \n",
    "len(x2_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(smooth_idf = False,sublinear_tf = False,norm = None,analyzer = 'word')\n",
    "cv = CountVectorizer()\n",
    "txt_fitted = tf.fit(x2_str)\n",
    "txt_transformed = txt_fitted.transform(x2_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 56704)\t6.807896460604202\n",
      "  (0, 50821)\t3.4893341712487405\n",
      "  (0, 46514)\t4.235598383576734\n",
      "  (0, 26540)\t4.747991443463681\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(\"The Text:\",x2_str)\n",
    "\n",
    "for i, row in enumerate(txt_transformed):\n",
    "    print(txt_transformed[i])\n",
    "    break\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(txt_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(881900, 76656)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 56704)\t6.807896460604202\n",
      "  (0, 50821)\t3.4893341712487405\n",
      "  (0, 46514)\t4.235598383576734\n",
      "  (0, 26540)\t4.747991443463681\n"
     ]
    }
   ],
   "source": [
    "print (txt_transformed[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text_vectorizer = TfidfVectorizer(min_df=4, max_features = 1000)\n",
    "test_vecor = text_vectorizer.fit_transform(x2_str)\n",
    "tfidf_vector = dict(zip(text_vectorizer.get_feature_names(), text_vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 7.451338242896674\n",
      "10 5.280889784947801\n",
      "100 6.295940108719296\n",
      "11 6.493949692476243\n",
      "12 5.7603997999876135\n",
      "13 7.189858542838917\n",
      "14 6.380896831195261\n",
      "15 6.921301782865005\n",
      "16 6.557716310835232\n",
      "18 6.61425244711932\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "for keys,values in tfidf_vector.items():\n",
    "    print (keys, values)\n",
    "    n+=1\n",
    "    if(n==10):break\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compound: 0.5719, \n",
      "neg: 0.0, \n",
      "neu: 0.448, \n",
      "pos: 0.552, \n",
      "exactly need perfect replacement\n",
      "compound: 0.6124, \n",
      "neg: 0.0, \n",
      "neu: 0.8, \n",
      "pos: 0.2, \n",
      "agree review opening small almost bend hook expensive earring try get high end see would buy price send back agree review opening\n",
      "compound: 0.8126, \n",
      "neg: 0.085, \n",
      "neu: 0.597, \n",
      "pos: 0.318, \n",
      "love go order another pack keep work someone include always lose back earring understand fish hook earring wish tiny bit long new friend\n",
      "compound: 0.0, \n",
      "neg: 0.0, \n",
      "neu: 1.0, \n",
      "pos: 0.0, \n",
      "tiny opening two star\n",
      "compound: 0.2263, \n",
      "neg: 0.0, \n",
      "neu: 0.513, \n",
      "pos: 0.487, \n",
      "okay three star\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "# sample_review = review_data.reviewcomment[:10]\n",
    "sample_review = x2_str[:5]\n",
    "for test in sample_review:\n",
    "    test\n",
    "    ss = analyser.polarity_scores(test)\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}, '.format(k, ss[k]))\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_transformed_df = pd.DataFrame.sparse.from_spmatrix(txt_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>76646</th>\n",
       "      <th>76647</th>\n",
       "      <th>76648</th>\n",
       "      <th>76649</th>\n",
       "      <th>76650</th>\n",
       "      <th>76651</th>\n",
       "      <th>76652</th>\n",
       "      <th>76653</th>\n",
       "      <th>76654</th>\n",
       "      <th>76655</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76656 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "\n",
       "   76646  76647  76648  76649  76650  76651  76652  76653  76654  76655  \n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 76656 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_transformed_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(txt_transformed_df.iloc[4])\n",
    "type(txt_transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_transformed_df['overall'] = pd_read['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt_transformed_df['asin'] = pd_read['asin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_transformed_df['reviewcomment'] = review_data['reviewcomment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>76649</th>\n",
       "      <th>76650</th>\n",
       "      <th>76651</th>\n",
       "      <th>76652</th>\n",
       "      <th>76653</th>\n",
       "      <th>76654</th>\n",
       "      <th>76655</th>\n",
       "      <th>overall</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewcomment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Exactly what I needed. perfect replacements!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>I agree with the other review, the opening is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Love these... I am going to order another pack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>too tiny an opening Two Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Okay Three Stars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76659 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  76649  76650  76651  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.0   \n",
       "\n",
       "   76652  76653  76654  76655  overall        asin  \\\n",
       "0    0.0    0.0    0.0    0.0        5  7106116521   \n",
       "1    0.0    0.0    0.0    0.0        2  7106116521   \n",
       "2    0.0    0.0    0.0    0.0        4  7106116521   \n",
       "3    0.0    0.0    0.0    0.0        2  7106116521   \n",
       "4    0.0    0.0    0.0    0.0        3  7106116521   \n",
       "\n",
       "                                       reviewcomment  \n",
       "0      Exactly what I needed. perfect replacements!!  \n",
       "1  I agree with the other review, the opening is ...  \n",
       "2  Love these... I am going to order another pack...  \n",
       "3                      too tiny an opening Two Stars  \n",
       "4                                   Okay Three Stars  \n",
       "\n",
       "[5 rows x 76659 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_transformed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [881900, 17]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-8d25b5d16f7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_transformed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2125\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2127\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \"\"\"\n\u001b[1;32m    292\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    257\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [881900, 17]"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(txt_transformed, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " five\n",
      " star\n",
      " great\n",
      " nice\n",
      " good\n",
      "\n",
      "Cluster 1:\n",
      " ring\n",
      " love\n",
      " beautiful\n",
      " look\n",
      " great\n",
      "\n",
      "Cluster 2:\n",
      " nice\n",
      " wear\n",
      " well\n",
      " fit\n",
      " get\n",
      "\n",
      "Cluster 3:\n",
      " beautiful\n",
      " five\n",
      " star\n",
      " dress\n",
      " love\n",
      "\n",
      "Cluster 4:\n",
      " perfect\n",
      " five\n",
      " fit\n",
      " star\n",
      " love\n",
      "\n",
      "Cluster 5:\n",
      " like\n",
      " look\n",
      " picture\n",
      " fit\n",
      " really\n",
      "\n",
      "Cluster 6:\n",
      " small\n",
      " size\n",
      " run\n",
      " two\n",
      " order\n",
      "\n",
      "Cluster 7:\n",
      " love\n",
      " five\n",
      " star\n",
      " daughter\n",
      " fit\n",
      "\n",
      "Cluster 8:\n",
      " large\n",
      " size\n",
      " order\n",
      " fit\n",
      " small\n",
      "\n",
      "Cluster 9:\n",
      " good\n",
      " quality\n",
      " price\n",
      " fit\n",
      " five\n",
      "\n",
      "Cluster 10:\n",
      " cute\n",
      " shirt\n",
      " super\n",
      " fit\n",
      " love\n",
      "\n",
      "Cluster 11:\n",
      " wife\n",
      " love\n",
      " five\n",
      " star\n",
      " great\n",
      "\n",
      "Cluster 12:\n",
      " excellent\n",
      " five\n",
      " star\n",
      " quality\n",
      " product\n",
      "\n",
      "Cluster 13:\n",
      " online\n",
      " look\n",
      " buy\n",
      " dress\n",
      " order\n",
      "\n",
      "Cluster 14:\n",
      " great\n",
      " fit\n",
      " look\n",
      " price\n",
      " love\n",
      "\n",
      "Cluster 15:\n",
      " three\n",
      " star\n",
      " small\n",
      " ok\n",
      " fit\n",
      "\n",
      "Cluster 16:\n",
      " one\n",
      " star\n",
      " small\n",
      " cheap\n",
      " break\n",
      "\n",
      "Cluster 17:\n",
      " four\n",
      " star\n",
      " nice\n",
      " good\n",
      " fit\n",
      "\n",
      "Cluster 18:\n",
      " love\n",
      " buy\n",
      " fit\n",
      " color\n",
      " get\n",
      "\n",
      "Cluster 19:\n",
      " happy\n",
      " purchase\n",
      " star\n",
      " five\n",
      " fit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "clusters = 20\n",
    "kmeans_model = MiniBatchKMeans(n_clusters=clusters, init='k-means++', n_init=1, \n",
    "                         init_size=1000, batch_size=1000, verbose=False, max_iter=1000)\n",
    "kmodel = kmeans_model.fit(test_vecor)\n",
    "kmodel_clusters = kmodel.predict(test_vecor)\n",
    "kmodel_distances = kmodel.transform(test_vecor)\n",
    "centroids = kmodel.cluster_centers_.argsort()[:, ::-1]\n",
    "values = text_vectorizer.get_feature_names()\n",
    "for i in range(clusters):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for j in centroids[i, :5]:\n",
    "        print(' %s' % values[j])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
