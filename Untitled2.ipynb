{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import json as json\n",
    "import seaborn as sbn\n",
    "# import pyspark as py\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "# import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: 465476, 2: 64718, 4: 149331, 3: 97031, 1: 107080}\n",
      "883636\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "d=dict()\n",
    "pd_read=pd.read_json('/ichec/work/mucom001c/Amazon/review/AMAZON_FASHION.json',lines=True)\n",
    "pd1=pd_read.values.tolist()\n",
    "for i in pd1:\n",
    "    t=i[0]\n",
    "    if t not in d:d[t]=1\n",
    "    else:\n",
    "        d[t]+=1\n",
    "    count+=1\n",
    "print(d)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall             int64\n",
       "verified             bool\n",
       "reviewTime         object\n",
       "reviewerID         object\n",
       "asin               object\n",
       "reviewerName       object\n",
       "reviewText         object\n",
       "summary            object\n",
       "unixReviewTime      int64\n",
       "vote              float64\n",
       "style              object\n",
       "image              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_read.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>style</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>10 20, 2014</td>\n",
       "      <td>A1D4G1SNUZWQOT</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Tracy</td>\n",
       "      <td>Exactly what I needed.</td>\n",
       "      <td>perfect replacements!!</td>\n",
       "      <td>1413763200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>09 28, 2014</td>\n",
       "      <td>A3DDWDH9PX2YX2</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Sonja Lau</td>\n",
       "      <td>I agree with the other review, the opening is ...</td>\n",
       "      <td>I agree with the other review, the opening is ...</td>\n",
       "      <td>1411862400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>08 25, 2014</td>\n",
       "      <td>A2MWC41EW7XL15</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Kathleen</td>\n",
       "      <td>Love these... I am going to order another pack...</td>\n",
       "      <td>My New 'Friends' !!</td>\n",
       "      <td>1408924800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>08 24, 2014</td>\n",
       "      <td>A2UH2QQ275NV45</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Jodi Stoner</td>\n",
       "      <td>too tiny an opening</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>1408838400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>07 27, 2014</td>\n",
       "      <td>A89F3LQADZBS5</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Alexander D.</td>\n",
       "      <td>Okay</td>\n",
       "      <td>Three Stars</td>\n",
       "      <td>1406419200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime      reviewerID        asin  reviewerName  \\\n",
       "0        5      True  10 20, 2014  A1D4G1SNUZWQOT  7106116521         Tracy   \n",
       "1        2      True  09 28, 2014  A3DDWDH9PX2YX2  7106116521     Sonja Lau   \n",
       "2        4     False  08 25, 2014  A2MWC41EW7XL15  7106116521      Kathleen   \n",
       "3        2      True  08 24, 2014  A2UH2QQ275NV45  7106116521   Jodi Stoner   \n",
       "4        3     False  07 27, 2014   A89F3LQADZBS5  7106116521  Alexander D.   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0                             Exactly what I needed.   \n",
       "1  I agree with the other review, the opening is ...   \n",
       "2  Love these... I am going to order another pack...   \n",
       "3                                too tiny an opening   \n",
       "4                                               Okay   \n",
       "\n",
       "                                             summary  unixReviewTime  vote  \\\n",
       "0                             perfect replacements!!      1413763200   NaN   \n",
       "1  I agree with the other review, the opening is ...      1411862400   3.0   \n",
       "2                                My New 'Friends' !!      1408924800   NaN   \n",
       "3                                          Two Stars      1408838400   NaN   \n",
       "4                                        Three Stars      1406419200   NaN   \n",
       "\n",
       "  style image  \n",
       "0   NaN   NaN  \n",
       "1   NaN   NaN  \n",
       "2   NaN   NaN  \n",
       "3   NaN   NaN  \n",
       "4   NaN   NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_read.shape\n",
    "\n",
    "import array as arr\n",
    "\n",
    "pd_read=pd.read_json('/ichec/work/mucom001c/Amazon/review/AMAZON_FASHION.json',lines=True)\n",
    "pd_read.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Exactly what I needed.</td>\n",
       "      <td>perfect replacements!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I agree with the other review, the opening is ...</td>\n",
       "      <td>I agree with the other review, the opening is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Love these... I am going to order another pack...</td>\n",
       "      <td>My New 'Friends' !!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>too tiny an opening</td>\n",
       "      <td>Two Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Okay</td>\n",
       "      <td>Three Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883631</th>\n",
       "      <td>5</td>\n",
       "      <td>I absolutely love this dress!!  It's sexy and ...</td>\n",
       "      <td>I absolutely love this dress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883632</th>\n",
       "      <td>5</td>\n",
       "      <td>I'm 5'6 175lbs. I'm on the tall side. I wear a...</td>\n",
       "      <td>I wear a large and ordered a large and it stil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883633</th>\n",
       "      <td>3</td>\n",
       "      <td>Too big in the chest area!</td>\n",
       "      <td>Three Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883634</th>\n",
       "      <td>3</td>\n",
       "      <td>Too clear in the back, needs lining</td>\n",
       "      <td>Three Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883635</th>\n",
       "      <td>5</td>\n",
       "      <td>Ordered and was slightly small. Worked with th...</td>\n",
       "      <td>The quality is excellent and it is so cute</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>883636 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall                                         reviewText  \\\n",
       "0             5                             Exactly what I needed.   \n",
       "1             2  I agree with the other review, the opening is ...   \n",
       "2             4  Love these... I am going to order another pack...   \n",
       "3             2                                too tiny an opening   \n",
       "4             3                                               Okay   \n",
       "...         ...                                                ...   \n",
       "883631        5  I absolutely love this dress!!  It's sexy and ...   \n",
       "883632        5  I'm 5'6 175lbs. I'm on the tall side. I wear a...   \n",
       "883633        3                         Too big in the chest area!   \n",
       "883634        3                Too clear in the back, needs lining   \n",
       "883635        5  Ordered and was slightly small. Worked with th...   \n",
       "\n",
       "                                                  summary  \n",
       "0                                  perfect replacements!!  \n",
       "1       I agree with the other review, the opening is ...  \n",
       "2                                     My New 'Friends' !!  \n",
       "3                                               Two Stars  \n",
       "4                                             Three Stars  \n",
       "...                                                   ...  \n",
       "883631                       I absolutely love this dress  \n",
       "883632  I wear a large and ordered a large and it stil...  \n",
       "883633                                        Three Stars  \n",
       "883634                                        Three Stars  \n",
       "883635         The quality is excellent and it is so cute  \n",
       "\n",
       "[883636 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data=pd_read.drop(['verified','reviewerID','asin','reviewTime','reviewerName','unixReviewTime','vote','style','image'],axis=1)\n",
    "review_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f44388c8130>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEGCAYAAACpXNjrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAASW0lEQVR4nO3df7CmdV3/8efLXQjTCJQT4S61TO1Um5XmfpGibzU4wWLmUoMMTspmFM0EDY79wm9NlsV8bcrMzJgYIdhqAkrNzShiEDUdEc4CokBMJ0LZHWw3QBAbdaB3f9yfZe8O5xwO+rnv65yzz8fMPXtd7+vH533u0Xlx/bivK1WFJEk9PWvoBiRJa4/hIknqznCRJHVnuEiSujNcJEndrR+6gZXimGOOqU2bNg3dhiStKrt37/7PqpqZXzdcmk2bNjE7Ozt0G5K0qiT59EJ1T4tJkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrrzF/qS1Mkf/+LfDd3CRFzw1h97xtt45CJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6m7i4ZJkXZLbkry/zZ+Q5ONJ5pJcneTwVv+aNj/Xlm8a28cbW/2eJKeN1be12lySi8bqC44hSZqOaRy5XAjcPTb/u8DbqupbgYeBc1v9XODhVn9bW48kW4Czge8EtgF/0gJrHfBO4HRgC/Dqtu5SY0iSpmCi4ZJkI/CjwLvafIBTgL9pq1wJnNGmt7d52vKXtfW3A1dV1Zeq6t+BOeDE9pmrqnur6svAVcD2pxlDkjQFkz5y+UPgV4D/bvPPBz5XVY+3+T3Ahja9AbgfoC1/pK3/ZH3eNovVlxrjf0lyXpLZJLP79+//Sv9GSdI8EwuXJK8A9lXV7kmN8dWqqkuramtVbZ2ZmRm6HUlaM9ZPcN8nA69M8nLgCOBI4O3AUUnWtyOLjcDetv5e4HhgT5L1wNcDD47VDxjfZqH6g0uMIUmagokduVTVG6tqY1VtYnRB/gNV9ZPAjcCZbbUdwPva9K42T1v+gaqqVj+73U12ArAZuBm4Bdjc7gw7vI2xq22z2BiSpCkY4ncuvwq8Ickco+sjl7X6ZcDzW/0NwEUAVXUncA1wF/CPwPlV9UQ7KrkAuI7R3WjXtHWXGkOSNAWTPC32pKr6IPDBNn0vozu95q/zReBVi2x/MXDxAvVrgWsXqC84hiRpOvyFviSpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6m5i4ZLkiCQ3J/lEkjuT/Farn5Dk40nmklyd5PBW/5o2P9eWbxrb1xtb/Z4kp43Vt7XaXJKLxuoLjiFJmo5JHrl8CTilqr4HeBGwLclJwO8Cb6uqbwUeBs5t658LPNzqb2vrkWQLcDbwncA24E+SrEuyDngncDqwBXh1W5clxpAkTcHEwqVGHmuzh7VPAacAf9PqVwJntOntbZ62/GVJ0upXVdWXqurfgTngxPaZq6p7q+rLwFXA9rbNYmNIkqZgotdc2hHG7cA+4Hrg34DPVdXjbZU9wIY2vQG4H6AtfwR4/nh93jaL1Z+/xBjz+zsvyWyS2f379381f6okacxEw6WqnqiqFwEbGR1pfPskx3umqurSqtpaVVtnZmaGbkeS1oyp3C1WVZ8DbgS+Dzgqyfq2aCOwt03vBY4HaMu/HnhwvD5vm8XqDy4xhiRpCiZ5t9hMkqPa9LOBHwHuZhQyZ7bVdgDva9O72jxt+Qeqqlr97HY32QnAZuBm4BZgc7sz7HBGF/13tW0WG0OSNAXrn36Vr9hxwJXtrq5nAddU1fuT3AVcleR3gNuAy9r6lwF/nmQOeIhRWFBVdya5BrgLeBw4v6qeAEhyAXAdsA64vKrubPv61UXGkCRNwcTCparuAF68QP1eRtdf5te/CLxqkX1dDFy8QP1a4NrljiFJmg5/oS9J6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktTdssIlyQ3LqUmSBE/zC/0kRwBfCxyT5GggbdGRLPIYe0mSnu7xLz8HvB54AbCbg+HyKPDHE+xLkrSKLRkuVfV24O1JfqGq3jGlniRJq9yyHlxZVe9I8v3ApvFtqmrnhPqSJK1iywqXJH8OfAtwO/BEKxdguEiSnmK5j9zfCmxpL+KSJGlJy/2dy6eAb5xkI5KktWO5Ry7HAHcluRn40oFiVb1yIl1Jkla15YbLb06yCUnS2rLcu8U+NOlGJElrx3LvFvs8o7vDAA4HDgO+UFVHTqoxSdLqtdwjl687MJ0kwHbgpEk1JUla3Z7xU5Fr5G+B0ybQjyRpDVjuabGfGJt9FqPfvXxxIh1Jkla95d4t9mNj048D9zE6NSZJ0lMs95rL6ybdiCRp7Vjuy8I2Jnlvkn3t8+4kGyfdnCRpdVruBf0/A3Yxeq/LC4C/azVJkp5iueEyU1V/VlWPt88VwMwE+5IkrWLLDZcHk7wmybr2eQ3w4CQbkyStXssNl58GzgI+CzwAnAn81IR6kiStcsu9FfnNwI6qehggyfOA32cUOpIk/S/LPXL57gPBAlBVDwEvnkxLkqTVbrnh8qwkRx+YaUcuyz3qkSQdYpYbEG8FPpbkr9v8q4CLJ9OSJGm1W+4v9HcmmQVOaaWfqKq7JteWJGk1W/aprRYmBook6Wk940fuL1eS45PcmOSuJHcmubDVn5fk+iT/2v49utWT5I+SzCW5I8n3ju1rR1v/X5PsGKu/JMkn2zZ/1N41s+gYkqTpmFi4MHp68i9W1RZGLxY7P8kW4CLghqraDNzQ5gFOBza3z3nAJfDkzQNvAl4KnAi8aSwsLgF+dmy7ba2+2BiSpCmYWLhU1QNVdWub/jxwN7CB0aP6r2yrXQmc0aa3Azvby8huAo5Kchyjl5JdX1UPtduhrwe2tWVHVtVNVVXAznn7WmgMSdIUTPLI5UlJNjH6XczHgWOr6oG26LPAsW16A3D/2GZ7Wm2p+p4F6iwxxvy+zksym2R2//79z/wPkyQtaOLhkuS5wLuB11fVo+PL2hFHTXL8pcaoqkuramtVbZ2Z8TmcktTLRMMlyWGMguUvq+o9rfwf7ZQW7d99rb4XOH5s842ttlR94wL1pcaQJE3BJO8WC3AZcHdV/cHYol3AgTu+dgDvG6uf0+4aOwl4pJ3aug44NcnR7UL+qcB1bdmjSU5qY50zb18LjSFJmoJJPsLlZOC1wCeT3N5q/w94C3BNknOBTzN62jLAtcDLgTngv4DXweg5Zkl+G7ilrffm9mwzgJ8HrgCeDfxD+7DEGJKkKZhYuFTVR4AssvhlC6xfwPmL7Oty4PIF6rPACxeoP7jQGJKk6ZjK3WKSpEOL4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpu0m+iVLSIeBDP/hDQ7cwET/04Q8N3cKq5pGLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1N3EXnOc5HLgFcC+qnphqz0PuBrYBNwHnFVVDycJ8Hbg5cB/AT9VVbe2bXYAv952+ztVdWWrvwS4Ang2cC1wYVXVYmN8NX/LS35551ez+Yq0+/fOGboFSWvYJI9crgC2zatdBNxQVZuBG9o8wOnA5vY5D7gEngyjNwEvBU4E3pTk6LbNJcDPjm237WnGkCRNycTCpao+DDw0r7wduLJNXwmcMVbfWSM3AUclOQ44Dbi+qh5qRx/XA9vasiOr6qaqKmDnvH0tNIYkaUqmfc3l2Kp6oE1/Fji2TW8A7h9bb0+rLVXfs0B9qTGeIsl5SWaTzO7fv/8r+HMkSQsZ7IJ+O+KoIceoqkuramtVbZ2ZmZlkK5J0SJnYBf1F/EeS46rqgXZqa1+r7wWOH1tvY6vtBX54Xv2Drb5xgfWXGkPq5uR3nDx0CxPx0V/46NAtaI2Y9pHLLmBHm94BvG+sfk5GTgIeaae2rgNOTXJ0u5B/KnBdW/ZokpPanWbnzNvXQmNIkqZkkrci/xWjo45jkuxhdNfXW4BrkpwLfBo4q61+LaPbkOcY3Yr8OoCqeijJbwO3tPXeXFUHbhL4eQ7eivwP7cMSY0iSpmRi4VJVr15k0csWWLeA8xfZz+XA5QvUZ4EXLlB/cKExJEnT4y/0JUndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO6m/VRkrWKfefN3Dd3CRHzTb3xy6BakNccjF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd2s2XJJsS3JPkrkkFw3djyQdStZkuCRZB7wTOB3YArw6yZZhu5KkQ8eaDBfgRGCuqu6tqi8DVwHbB+5Jkg4Zqaqhe+guyZnAtqr6mTb/WuClVXXBvPXOA85rs98G3DPVRp/qGOA/B+5hpfC7OMjv4iC/i4NWynfxzVU1M7+4fohOVoqquhS4dOg+DkgyW1Vbh+5jJfC7OMjv4iC/i4NW+nexVk+L7QWOH5vf2GqSpClYq+FyC7A5yQlJDgfOBnYN3JMkHTLW5Gmxqno8yQXAdcA64PKqunPgtpZjxZyiWwH8Lg7yuzjI7+KgFf1drMkL+pKkYa3V02KSpAEZLpKk7gyXFSDJ5Un2JfnU0L0MLcnxSW5McleSO5NcOHRPQ0lyRJKbk3yifRe/NXRPQ0qyLsltSd4/dC9DS3Jfkk8muT3J7ND9LMRrLitAkh8EHgN2VtULh+5nSEmOA46rqluTfB2wGzijqu4auLWpSxLgOVX1WJLDgI8AF1bVTQO3NogkbwC2AkdW1SuG7mdISe4DtlbVSvgR5YI8clkBqurDwEND97ESVNUDVXVrm/48cDewYdiuhlEjj7XZw9rnkPyvwSQbgR8F3jV0L1oew0UrVpJNwIuBjw/byXDaqaDbgX3A9VV1qH4Xfwj8CvDfQzeyQhTwT0l2t8dYrTiGi1akJM8F3g28vqoeHbqfoVTVE1X1IkZPmTgxySF32jTJK4B9VbV76F5WkB+oqu9l9OT389up9RXFcNGK064vvBv4y6p6z9D9rARV9TngRmDb0L0M4GTgle06w1XAKUn+YtiWhlVVe9u/+4D3MnoS/IpiuGhFaRexLwPurqo/GLqfISWZSXJUm3428CPAvwzb1fRV1RuramNVbWL0KKcPVNVrBm5rMEme0252IclzgFOBFXenqeGyAiT5K+BjwLcl2ZPk3KF7GtDJwGsZ/dfp7e3z8qGbGshxwI1J7mD0vLzrq+qQvw1XHAt8JMkngJuBv6+qfxy4p6fwVmRJUnceuUiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0VaY5J8MMnWNn1fkmOG7kmHHsNFWmUy4v93taL5P1BpCpK8Icmn2uf1Sd6S5Pyx5b+Z5Jfa9C8nuSXJHQfe4ZJkU5J7kuxk9Gvs45NckmTWd71oJVo/dAPSWpfkJcDrgJcCYfSU59cwetLvO9tqZwGnJTkV2MzoWVEBdrWHEn6m1XcceJ9Lkl+rqoeSrANuSPLdVXXHFP80aVGGizR5PwC8t6q+AJDkPcD/Bb4hyQuAGeDhqrq/vXnzVOC2tu1zGYXKZ4BPz3tR2FntcevrGT0qZgtguGhFMFyk4fw1cCbwjcDVrRbg/1fVn46v2N5t84Wx+ROAXwL+T1U9nOQK4IjJtywtj9dcpMn7Z+CMJF/bnmL74612NaOn/J7JKGgArgN+ur3PhiQbknzDAvs8klHYPJLkWEbv9ZBWDI9cpAmrqlvbkcXNrfSuqroNoD06fW9VPdDW/ack3wF8bPT2AR5jdH3miXn7/ESS2xg9gv9+4KPT+Fuk5fKpyJKk7jwtJknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKm7/wFnnE7qnO5NHAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sbn.countplot(review_data['overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall          0\n",
      "reviewText    1233\n",
      "summary        533\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(review_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean( rev, remove_stopwords=True): \n",
    "    new_text = re.sub(\"[^a-zA-Z]\",\" \", rev)\n",
    "    words = new_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        sts = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in sts]\n",
    "    ary=[]\n",
    "    eng_stemmer = english_stemmer \n",
    "    for word in words:\n",
    "        ary.append(eng_stemmer.stem(word))\n",
    "    return(ary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (883636, 3)\n",
      "Drop Nulls: (881900, 3)\n",
      "overall       0\n",
      "reviewText    0\n",
      "summary       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# clean_reviewData = []\n",
    "# for rev in review_data['reviewText']:\n",
    "#     clean_reviewData.append( \" \".join(data_clean(rev)))\n",
    "    \n",
    "# clean_summaryData = []\n",
    "# for rev in review_data['summary']:\n",
    "#     clean_summaryData.append( \" \".join(data_clean(rev)))\n",
    "\n",
    "\n",
    "print (\"Original:\", review_data.shape)\n",
    "# review_data_dd = review_data.drop_duplicates()\n",
    "# dd = review_data_dd.reset_index(drop=True)\n",
    "# print (\"Drop Dupicates:\", dd.shape)\n",
    "review_data = review_data.dropna()\n",
    "review_data = review_data.reset_index(drop=True)\n",
    "print (\"Drop Nulls:\", review_data.shape)\n",
    "print(review_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review_data[\"reviewcomment\"] = review_data[\"reviewText\"] + review_data[\"summary\"]\n",
    "review_data['reviewcomment'] = review_data[['reviewText', 'summary']].astype(str).apply(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewcomment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Exactly what I needed. perfect replacements!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I agree with the other review, the opening is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Love these... I am going to order another pack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>too tiny an opening Two Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Okay Three Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881895</th>\n",
       "      <td>5</td>\n",
       "      <td>I absolutely love this dress!!  It's sexy and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881896</th>\n",
       "      <td>5</td>\n",
       "      <td>I'm 5'6 175lbs. I'm on the tall side. I wear a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881897</th>\n",
       "      <td>3</td>\n",
       "      <td>Too big in the chest area! Three Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881898</th>\n",
       "      <td>3</td>\n",
       "      <td>Too clear in the back, needs lining Three Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881899</th>\n",
       "      <td>5</td>\n",
       "      <td>Ordered and was slightly small. Worked with th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>881900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall                                      reviewcomment\n",
       "0             5      Exactly what I needed. perfect replacements!!\n",
       "1             2  I agree with the other review, the opening is ...\n",
       "2             4  Love these... I am going to order another pack...\n",
       "3             2                      too tiny an opening Two Stars\n",
       "4             3                                   Okay Three Stars\n",
       "...         ...                                                ...\n",
       "881895        5  I absolutely love this dress!!  It's sexy and ...\n",
       "881896        5  I'm 5'6 175lbs. I'm on the tall side. I wear a...\n",
       "881897        3             Too big in the chest area! Three Stars\n",
       "881898        3    Too clear in the back, needs lining Three Stars\n",
       "881899        5  Ordered and was slightly small. Worked with th...\n",
       "\n",
       "[881900 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_list=review_data.drop(['reviewText','summary'],axis=1)\n",
    "review_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data_list = review_data_list['reviewcomment'].values.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I agree with the other review, the opening is too small.  I almost bent the hook on some very expensive earrings trying to get these up higher than just the end so they're not seen.  Would not buy again but for the price, not sending back. I agree with the other review, the opening is ...\n"
     ]
    }
   ],
   "source": [
    "print((review_data_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "\n",
    "token_list = list()\n",
    "\n",
    "for i in (review_data_list):\n",
    "    cleaned = re.sub('\\W+', ' ', i).lower()\n",
    "    cleaned = cleaned.strip()\n",
    "    token_list.append(word_tokenize(str(cleaned)))\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['exactly', 'what', 'i', 'needed', 'perfect', 'replacements'], ['i', 'agree', 'with', 'the', 'other', 'review', 'the', 'opening', 'is', 'too', 'small', 'i', 'almost', 'bent', 'the', 'hook', 'on', 'some', 'very', 'expensive', 'earrings', 'trying', 'to', 'get', 'these', 'up', 'higher', 'than', 'just', 'the', 'end', 'so', 'they', 're', 'not', 'seen', 'would', 'not', 'buy', 'again', 'but', 'for', 'the', 'price', 'not', 'sending', 'back', 'i', 'agree', 'with', 'the', 'other', 'review', 'the', 'opening', 'is'], ['love', 'these', 'i', 'am', 'going', 'to', 'order', 'another', 'pack', 'to', 'keep', 'in', 'work', 'someone', 'including', 'myself', 'is', 'always', 'losing', 'the', 'back', 'to', 'an', 'earring', 'i', 'don', 't', 'understand', 'why', 'all', 'fish', 'hook', 'earrings', 'don', 't', 'have', 'them', 'just', 'wish', 'that', 'they', 'were', 'a', 'tiny', 'bit', 'longer', 'my', 'new', 'friends'], ['too', 'tiny', 'an', 'opening', 'two', 'stars'], ['okay', 'three', 'stars'], ['exactly', 'what', 'i', 'wanted', 'five', 'stars'], ['these', 'little', 'plastic', 'backs', 'work', 'great', 'no', 'more', 'loosing', 'hook', 'earrings', 'wish', 'i', 'had', 'ordered', 'these', 'sooner', 'before', 'i', 'had', 'lost', 'some', 'of', 'my', 'favorite', 'earrings', 'works', 'great'], ['mother', 'in', 'law', 'wanted', 'it', 'as', 'a', 'present', 'for', 'her', 'sister', 'she', 'liked', 'it', 'and', 'said', 'it', 'would', 'work', 'bought', 'as', 'a', 'present'], ['item', 'is', 'of', 'good', 'quality', 'looks', 'great', 'too', 'but', 'it', 'does', 'not', 'fit', '100', 's', 'but', 'it', 'can', 'be', 'stretched', 'to', 'fit', 'them', 'if', 'you', 'carefully', 'push', 'bottom', 'of', 'case', 'with', 'your', 'fingers', 'then', 'shove', 'in', 'a', 'pack', 'of', '100', 's', 'max', 'and', 'level', 'it', 'out', 'as', 'you', 'close', 'case', 'stretching', 'case', 'closed', 'leave', 'a', 'pk', 'in', 'it', 'for', 'month', 'or', 'so', 'was', 'la', 'buxton', 'is', 'usually', 'a', 'good', 'quality', 'product', 'buxton', 'heiress', 'collection'], ['i', 'had', 'used', 'my', 'last', 'el', 'cheapo', 'fake', 'leather', 'cigarette', 'case', 'for', 'seven', 'years', 'it', 'still', 'closed', 'completely', 'but', 'the', 'plastic', 'made', 'to', 'look', 'like', 'leather', 'was', 'literally', 'falling', 'off', 'so', 'it', 'was', 'time', 'for', 'a', 'new', 'one', 'cigarette', 'cases', 'for', 'kings', 'size', 'cigs', 'are', 'not', 'easy', 'to', 'come', 'by', 'these', 'days', 'i', 'discovered', 'but', 'i', 'was', 'thrilled', 'to', 'find', 'this', 'one', 'on', 'amazon', 'it', 'was', 'a', 'great', 'price', 'real', 'leather', 'and', 'even', 'had', 'the', 'cool', 'zipper', 'pouch', 'on', 'the', 'back', 'i', 'was', 'so', 'excited', 'to', 'get', 'my', 'case', 'and', 'toss', 'that', 'other', 'one', 'well', 'within', 'three', 'days', 'one', 'of', 'the', 'gold', 'clasps', 'literally', 'broke', 'off', 'i', 'couldn', 't', 'believe', 'it', 'i', 'tried', 'to', 'super', 'glue', 'it', 'back', 'on', 'and', 'was', 'not', 'successful', 'so', 'i', 'still', 'use', 'the', 'case', 'but', 'it', 'doesn', 't', 'close', 'securely', 'i', 'was', 'very', 'disappointed', 'that', 'my', '3', '00', 'plastic', 'one', 'lasted', '7', 'years', 'and', 'this', 'real', 'nice', 'leather', 'one', 'lasted', '3', 'days', 'but', 'i', 'still', 'love', 'the', 'zipper', 'pouch', 'on', 'the', 'back', 'it', 's', 'great', 'for', 'the', 'spare', 'key', 'to', 'my', 'car', 'because', 'i', 'will', 'not', 'go', 'anywhere', 'without', 'my', 'cigarettes', 'top', 'clasp', 'broke', 'within', '3', 'days']]\n"
     ]
    }
   ],
   "source": [
    "print(token_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list_copy = token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "881900"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_list_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "from nltk import PorterStemmer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Stop_Words(Sent):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_words = []\n",
    "    for w in Sent:\n",
    "        if w not in stop_words:\n",
    "            filtered_words.append(w)\n",
    "    return stop_words,filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=list()\n",
    "# count=0\n",
    "for word in token_list_copy:\n",
    "#     if count==1000:\n",
    "#         print(x1)\n",
    "#         break\n",
    "#     else:\n",
    "    x,y=Remove_Stop_Words(word)\n",
    "    x1.append(y)\n",
    "#         count+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words,filtered_words = Remove_Stop_Words(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilaise stemmer\n",
    "\n",
    "porter_stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['exactly', 'needed', 'perfect', 'replacements'], ['agree', 'review', 'opening', 'small', 'almost', 'bent', 'hook', 'expensive', 'earrings', 'trying', 'get', 'higher', 'end', 'seen', 'would', 'buy', 'price', 'sending', 'back', 'agree', 'review', 'opening'], ['love', 'going', 'order', 'another', 'pack', 'keep', 'work', 'someone', 'including', 'always', 'losing', 'back', 'earring', 'understand', 'fish', 'hook', 'earrings', 'wish', 'tiny', 'bit', 'longer', 'new', 'friends'], ['tiny', 'opening', 'two', 'stars'], ['okay', 'three', 'stars'], ['exactly', 'wanted', 'five', 'stars'], ['little', 'plastic', 'backs', 'work', 'great', 'loosing', 'hook', 'earrings', 'wish', 'ordered', 'sooner', 'lost', 'favorite', 'earrings', 'works', 'great'], ['mother', 'law', 'wanted', 'present', 'sister', 'liked', 'said', 'would', 'work', 'bought', 'present'], ['item', 'good', 'quality', 'looks', 'great', 'fit', '100', 'stretched', 'fit', 'carefully', 'push', 'bottom', 'case', 'fingers', 'shove', 'pack', '100', 'max', 'level', 'close', 'case', 'stretching', 'case', 'closed', 'leave', 'pk', 'month', 'la', 'buxton', 'usually', 'good', 'quality', 'product', 'buxton', 'heiress', 'collection'], ['used', 'last', 'el', 'cheapo', 'fake', 'leather', 'cigarette', 'case', 'seven', 'years', 'still', 'closed', 'completely', 'plastic', 'made', 'look', 'like', 'leather', 'literally', 'falling', 'time', 'new', 'one', 'cigarette', 'cases', 'kings', 'size', 'cigs', 'easy', 'come', 'days', 'discovered', 'thrilled', 'find', 'one', 'amazon', 'great', 'price', 'real', 'leather', 'even', 'cool', 'zipper', 'pouch', 'back', 'excited', 'get', 'case', 'toss', 'one', 'well', 'within', 'three', 'days', 'one', 'gold', 'clasps', 'literally', 'broke', 'believe', 'tried', 'super', 'glue', 'back', 'successful', 'still', 'use', 'case', 'close', 'securely', 'disappointed', '3', '00', 'plastic', 'one', 'lasted', '7', 'years', 'real', 'nice', 'leather', 'one', 'lasted', '3', 'days', 'still', 'love', 'zipper', 'pouch', 'back', 'great', 'spare', 'key', 'car', 'go', 'anywhere', 'without', 'cigarettes', 'top', 'clasp', 'broke', 'within', '3', 'days']]\n"
     ]
    }
   ],
   "source": [
    "# stem raw words with noise\n",
    "raw_words = (x1)\n",
    "print(raw_words[0:10])\n",
    "stemmed_words = [porter_stemmer.stem(word =' '.join(word)) for word in raw_words]\n",
    "stemdf = pd.DataFrame({'raw_word': raw_words,'stemmed_word': stemmed_words})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            raw_word  \\\n",
      "0           [exactly, needed, perfect, replacements]   \n",
      "1  [agree, review, opening, small, almost, bent, ...   \n",
      "2  [love, going, order, another, pack, keep, work...   \n",
      "3                        [tiny, opening, two, stars]   \n",
      "4                               [okay, three, stars]   \n",
      "\n",
      "                                        stemmed_word  \n",
      "0                      exactly needed perfect replac  \n",
      "1  agree review opening small almost bent hook ex...  \n",
      "2  love going order another pack keep work someon...  \n",
      "3                              tiny opening two star  \n",
      "4                                    okay three star  \n"
     ]
    }
   ],
   "source": [
    "print(stemdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             exactly needed perfect replac\n",
       "1         agree review opening small almost bent hook ex...\n",
       "2         love going order another pack keep work someon...\n",
       "3                                     tiny opening two star\n",
       "4                                           okay three star\n",
       "                                ...                        \n",
       "881895    absolutely love dress sexy comfortable split b...\n",
       "881896    5 6 175lbs tall side wear large ordered large ...\n",
       "881897                            big chest area three star\n",
       "881898                   clear back needs lining three star\n",
       "881899    ordered slightly small worked company gracious...\n",
       "Name: stemmed_word, Length: 881900, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemdf['stemmed_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  \n",
    "  pos_counts = Counter()\n",
    "\n",
    "\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lemmatizing_Words(Words):\n",
    "    Lm = WordNetLemmatizer()\n",
    "    Lemmatized_Words = []\n",
    "    for word in Words:\n",
    "        Lemmatized_Words.append(Lm.lemmatize(word,get_part_of_speech(word)))\n",
    "    return Lemmatized_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=list()\n",
    "for word in x1:\n",
    "#     print(word)\n",
    "    x_temp = Lemmatizing_Words(word)\n",
    "    x2.append(x_temp)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['exactly', 'needed', 'perfect', 'replacements'], ['agree', 'review', 'opening', 'small', 'almost', 'bent', 'hook', 'expensive', 'earrings', 'trying', 'get', 'higher', 'end', 'seen', 'would', 'buy', 'price', 'sending', 'back', 'agree', 'review', 'opening'], ['love', 'going', 'order', 'another', 'pack', 'keep', 'work', 'someone', 'including', 'always', 'losing', 'back', 'earring', 'understand', 'fish', 'hook', 'earrings', 'wish', 'tiny', 'bit', 'longer', 'new', 'friends'], ['tiny', 'opening', 'two', 'stars'], ['okay', 'three', 'stars'], ['exactly', 'wanted', 'five', 'stars'], ['little', 'plastic', 'backs', 'work', 'great', 'loosing', 'hook', 'earrings', 'wish', 'ordered', 'sooner', 'lost', 'favorite', 'earrings', 'works', 'great'], ['mother', 'law', 'wanted', 'present', 'sister', 'liked', 'said', 'would', 'work', 'bought', 'present'], ['item', 'good', 'quality', 'looks', 'great', 'fit', '100', 'stretched', 'fit', 'carefully', 'push', 'bottom', 'case', 'fingers', 'shove', 'pack', '100', 'max', 'level', 'close', 'case', 'stretching', 'case', 'closed', 'leave', 'pk', 'month', 'la', 'buxton', 'usually', 'good', 'quality', 'product', 'buxton', 'heiress', 'collection'], ['used', 'last', 'el', 'cheapo', 'fake', 'leather', 'cigarette', 'case', 'seven', 'years', 'still', 'closed', 'completely', 'plastic', 'made', 'look', 'like', 'leather', 'literally', 'falling', 'time', 'new', 'one', 'cigarette', 'cases', 'kings', 'size', 'cigs', 'easy', 'come', 'days', 'discovered', 'thrilled', 'find', 'one', 'amazon', 'great', 'price', 'real', 'leather', 'even', 'cool', 'zipper', 'pouch', 'back', 'excited', 'get', 'case', 'toss', 'one', 'well', 'within', 'three', 'days', 'one', 'gold', 'clasps', 'literally', 'broke', 'believe', 'tried', 'super', 'glue', 'back', 'successful', 'still', 'use', 'case', 'close', 'securely', 'disappointed', '3', '00', 'plastic', 'one', 'lasted', '7', 'years', 'real', 'nice', 'leather', 'one', 'lasted', '3', 'days', 'still', 'love', 'zipper', 'pouch', 'back', 'great', 'spare', 'key', 'car', 'go', 'anywhere', 'without', 'cigarettes', 'top', 'clasp', 'broke', 'within', '3', 'days']]\n"
     ]
    }
   ],
   "source": [
    "print(x1[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['exactly', 'need', 'perfect', 'replacement'], ['agree', 'review', 'opening', 'small', 'almost', 'bend', 'hook', 'expensive', 'earring', 'try', 'get', 'high', 'end', 'see', 'would', 'buy', 'price', 'send', 'back', 'agree', 'review', 'opening'], ['love', 'go', 'order', 'another', 'pack', 'keep', 'work', 'someone', 'include', 'always', 'lose', 'back', 'earring', 'understand', 'fish', 'hook', 'earring', 'wish', 'tiny', 'bit', 'long', 'new', 'friend'], ['tiny', 'opening', 'two', 'star']]\n"
     ]
    }
   ],
   "source": [
    "print(x2[0:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "881900\n"
     ]
    }
   ],
   "source": [
    "print (len(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "881900"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2_str = [' '.join(i) for i in x2] \n",
    "len(x2_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(smooth_idf = False,sublinear_tf = False,norm = None,analyzer = 'word')\n",
    "\n",
    "cv = CountVectorizer()\n",
    "txt_fitted = tf.fit(x2_str)\n",
    "txt_transformed = txt_fitted.transform(x2_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 56704)\t6.807896460604202\n",
      "  (0, 50821)\t3.4893341712487405\n",
      "  (0, 46514)\t4.235598383576734\n",
      "  (0, 26540)\t4.747991443463681\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(\"The Text:\",x2_str)\n",
    "\n",
    "for i, row in enumerate(txt_transformed):\n",
    "    print(txt_transformed[i])\n",
    "    break\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(txt_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(881900, 76656)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 56704)\t6.807896460604202\n",
      "  (0, 50821)\t3.4893341712487405\n",
      "  (0, 46514)\t4.235598383576734\n",
      "  (0, 26540)\t4.747991443463681\n"
     ]
    }
   ],
   "source": [
    "print (txt_transformed[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 504. GiB for an array with shape (881900, 76656) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-425207be94f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtxt_transformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output array must be C or F contiguous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 504. GiB for an array with shape (881900, 76656) and data type float64"
     ]
    }
   ],
   "source": [
    "txt_transformed.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [881900, 17]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-8d25b5d16f7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_transformed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2125\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2127\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \"\"\"\n\u001b[1;32m    292\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    257\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [881900, 17]"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(txt_transformed, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'m', 'too', 'ours', 'did', 'with', 'what', 'off', 'them', \"don't\", 'few', \"you're\", 'when', 'both', 'more', \"mightn't\", 'doing', 'yours', 'am', 'here', 'should', 'by', 'having', 'wasn', 'will', 'any', 'into', \"it's\", 'once', \"mustn't\", 'they', 'theirs', 'same', \"hadn't\", 'an', 'that', 'yourselves', 'over', 'why', \"wouldn't\", \"aren't\", 'weren', 'aren', 'such', 'a', 'there', 'mightn', 'o', 'most', \"you've\", \"shan't\", 'ourselves', 'herself', 'after', 'wouldn', 'not', \"hasn't\", 'no', 'didn', 'up', 'other', 'if', 'our', \"you'd\", 'mustn', \"that'll\", 'itself', 'we', 'how', 'just', 'is', 'ma', 'each', 's', 'being', \"didn't\", \"should've\", 'which', 'whom', 'of', 'for', 'above', 'in', \"doesn't\", 'some', 'very', 'y', 'before', 'as', 't', 'or', \"wasn't\", 'and', 'she', 'll', 'until', 'own', 'under', 'so', 'about', 'has', 'needn', 'now', 'doesn', 'these', 'during', 'on', 'yourself', 'had', 'me', 'himself', 'who', 'be', 'but', 'are', 'he', 'haven', 're', \"isn't\", 'my', 'isn', 'while', 'from', 'his', 'below', 'don', 'all', 'can', 'nor', 'because', 'those', 'where', 'shan', 'won', 'i', 'her', 'again', 'hers', \"she's\", 'than', 've', 'couldn', 'between', 'myself', 'through', \"won't\", 'hasn', 'have', 'been', 'down', 'your', 'their', 'were', 'themselves', 'out', \"weren't\", 'shouldn', 'hadn', 'further', 'then', \"needn't\", \"you'll\", 'you', 'against', 'only', \"shouldn't\", 'was', 'to', \"haven't\", \"couldn't\", 'the', 'him', 'its', 'this', 'at', 'ain', 'it', 'd', 'does', 'do'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SnowballStemmer('english')\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-99-0d46df939fa0>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-99-0d46df939fa0>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    stemmed_words = porter_stemmer.stem(word =' '.join(word))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def sentencePorterStem(sentence):\n",
    " \n",
    "    stem_sentence=[]\n",
    "    for word in sentence:\n",
    "        if word not in stop_words:\n",
    "#             return stem_sentence.append(ps.stem(word = ' '.join(word)))\n",
    "        stemmed_words = porter_stemmer.stem(word =' '.join(word))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'agree', 'with', 'the', 'other', 'review', 'the', 'opening', 'is', 'too', 'small', 'i', 'almost', 'bent', 'the', 'hook', 'on', 'some', 'very', 'expensive', 'earrings', 'trying', 'to', 'get', 'these', 'up', 'higher', 'than', 'just', 'the', 'end', 'so', 'they', 're', 'not', 'seen', 'would', 'not', 'buy', 'again', 'but', 'for', 'the', 'price', 'not', 'sending', 'back', 'i', 'agree', 'with', 'the', 'other', 'review', 'the', 'opening', 'is']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sen= str(sentiment_df['text'][3])\n",
    "sen = tuple(raw_words)\n",
    "print(raw_words[1])\n",
    "len(raw_words)\n",
    "type(sen)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps_sen = sentencePorterStem(sen)\n",
    "# ss_sen= sentenceSnowballStem(sen)\n",
    "# print(sen)\n",
    "# print('Porter Stem- '+ ps_sen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ps_sen[10])\n",
    "# print(stem_sentence[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'m', 'too', 'ours', 'did', 'with', 'what', 'off', 'them', \"don't\", 'few', \"you're\", 'when', 'both', 'more', \"mightn't\", 'doing', 'yours', 'am', 'here', 'should', 'by', 'having', 'wasn', 'will', 'any', 'into', \"it's\", 'once', \"mustn't\", 'they', 'theirs', 'same', \"hadn't\", 'an', 'that', 'yourselves', 'over', 'why', \"wouldn't\", \"aren't\", 'weren', 'aren', 'such', 'a', 'there', 'mightn', 'o', 'most', \"you've\", \"shan't\", 'ourselves', 'herself', 'after', 'wouldn', 'not', \"hasn't\", 'no', 'didn', 'up', 'other', 'if', 'our', \"you'd\", 'mustn', \"that'll\", 'itself', 'we', 'how', 'just', 'is', 'ma', 'each', 's', 'being', \"didn't\", \"should've\", 'which', 'whom', 'of', 'for', 'above', 'in', \"doesn't\", 'some', 'very', 'y', 'before', 'as', 't', 'or', \"wasn't\", 'and', 'she', 'll', 'until', 'own', 'under', 'so', 'about', 'has', 'needn', 'now', 'doesn', 'these', 'during', 'on', 'yourself', 'had', 'me', 'himself', 'who', 'be', 'but', 'are', 'he', 'haven', 're', \"isn't\", 'my', 'isn', 'while', 'from', 'his', 'below', 'don', 'all', 'can', 'nor', 'because', 'those', 'where', 'shan', 'won', 'i', 'her', 'again', 'hers', \"she's\", 'than', 've', 'couldn', 'between', 'myself', 'through', \"won't\", 'hasn', 'have', 'been', 'down', 'your', 'their', 'were', 'themselves', 'out', \"weren't\", 'shouldn', 'hadn', 'further', 'then', \"needn't\", \"you'll\", 'you', 'against', 'only', \"shouldn't\", 'was', 'to', \"haven't\", \"couldn't\", 'the', 'him', 'its', 'this', 'at', 'ain', 'it', 'd', 'does', 'do'}\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', '[', \"'\", 'e', 'x', 'c', 'l', \"'\", ',', ' ']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencePorterStem(sentence):\n",
    " \n",
    "    stem_sentence=[]\n",
    "    for word in sentence:\n",
    "        print(word)\n",
    "        if word not in stop_words:\n",
    "            print(word)\n",
    "            \n",
    "#             return stem_sentence.append(ps.stem(word = ' '.join(word)))\n",
    "#         stemmed_words = porter_stemmer.stem(word =' '.join(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "agree\n",
      "agree\n"
     ]
    }
   ],
   "source": [
    "sentencePorterStem((token_list_copy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'agree', 'with', 'the', 'other', 'review', 'the', 'opening', 'is', 'too', 'small', 'i', 'almost', 'bent', 'the', 'hook', 'on', 'some', 'very', 'expensive', 'earrings', 'trying', 'to', 'get', 'these', 'up', 'higher', 'than', 'just', 'the', 'end', 'so', 'they', 're', 'not', 'seen', 'would', 'not', 'buy', 'again', 'but', 'for', 'the', 'price', 'not', 'sending', 'back', 'i', 'agree', 'with', 'the', 'other', 'review', 'the', 'opening', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(token_list_copy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check for extra symbols\n",
    "extra_symbol = []\n",
    "for i in str(token_list_copy):\n",
    "    extra_symbol.append(re.match(r\"[,@\\'?\\.$%_!^]\",i))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, <re.Match object; span=(0, 1), match=\"'\">, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "print(extra_symbol[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_word_list = token_list_copy[:] #make a copy of the word_list\n",
    "for word in token_list_copy: # iterate over word_list\n",
    "    if word not in stopwords.words('english'): \n",
    "        filtered_word_list.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "881900\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
